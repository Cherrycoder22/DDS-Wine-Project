---
title: "Wine Analysis"
output: html_document
date: "2024-12-15"
---
```{r setup, include=FALSE}
library(caret)
library(randomForest)
#Read Datasets
wine_test <-read.csv("/Users/cherokeecarr/Desktop/Wine Test Set.csv")
print(wine_test)
wine_train <- read.csv("/Users/cherokeecarr/Desktop/Wine Train.csv")
print(wine_train)
# Combining datasets vertically (adding rows)
missing_columns <- setdiff(colnames(wine_train), colnames(wine_test))
wine_test[missing_columns] <- NA
colnames(wine_test) <- colnames(wine_train)


Vino <- rbind(wine_train, wine_test)

Vino <- na.omit(Vino)

# Train the Random Forest Model
rf_model <- randomForest(quality ~ density + pH + fixed.acidity + volatile.acidity +
                           residual.sugar + citric.acid + sulphates + alcohol,
                         data = Vino)

# Make predictions using the Random Forest model
rf_predictions <- predict(rf_model, newdata = wine_test)
print(rf_predictions)

# Ensure that quality column in wine_test has the same factor levels as in training data
wine_train$quality <- factor(wine_train$quality, levels = levels(Vino$quality))

# Make predictions using the Linear Model
lm_predictions <- predict(lm_model, newdata = wine_test)
print(lm_predictions)

# Calculate MAE for both models
lm_mae <- mean(abs(lm_predictions - wine_test$quality))
cat("Linear Model MAE:", lm_mae, "\n")

rf_mae <- mean(abs(rf_predictions - wine_test$quality))
cat("Random Forest MAE:", rf_mae, "\n")

lm_model <- lm(alcohol ~ density + pH  + fixed.acidity + volatile.acidity +residual.sugar, data = wine_train)
print(lm_model)

# Make predictions on the test set (wine_test should be the actual test data)
lm_predictions <- predict(lm_model, newdata = wine_test)
rf_predictions <- predict(rf_model, newdata = wine_test)

# Calculate MAE for Linear Regression
lm_mae <- mean(abs(lm_predictions - wine_test$alcohol))
cat("Linear Regression MAE:", lm_mae, "\n")

# Calculate MAE for Random Forest
rf_mae <- mean(abs(rf_predictions - wine_test$alcohol))
cat("Random Forest MAE:", rf_mae, "\n")

# Compare which model has a lower MAE
if (lm_mae < rf_mae) {
  cat("Linear Regression performs better with a lower MAE:", lm_mae, "\n")
} else {
  cat("Random Forest performs better with a lower MAE:", rf_mae, "\n")
}

```

Analysis A, we were tasked to build a predictive model with the lowest possible MAE. I built two different models to compare. One, with as many relevant predictors as possible and the other as a simplified version. One comparing the two, lm_mae has the lower MAE with .446. The key to a lower MAE is to assure the predictors are actually building the model and not being used as filler. As well as making a simple model with as much predictable information as possible.




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(e1071)  # For naiveBayes
library(caret)
wine_test <-read.csv("/Users/cherokeecarr/Desktop/Wine Test Set.csv")
print(wine_test)
wine_train <- read.csv("/Users/cherokeecarr/Desktop/Wine Train.csv")
print(wine_train)


quality_9_wines <- subset(wine_train, quality == 9)

# Check the first few rows of the filtered dataset
head(quality_9_wines)

quality_8_wines <- subset(wine_train, quality == 8)
print(quality_8_wines)

```

## R Markdown

Analysis B, we were tasked to find the key factors in making high-quality wine. To start researching them, we decided to see what factors influence the quality 8 and 9 wines. 
After seeing those determinants, we decided to create a prediction on those models, coded below.

The strongest key factors were density, alchohol content, sugar level, fixed acidity, and volatile acidity. To help build model, we also included ph and citric acid even though they are not the strongest predictors. 

```{r}
library(caret)       # For confusionMatrix
library(e1071)       # For Naive Bayes

# Load the datasets
wine_test <- read.csv("/Users/cherokeecarr/Desktop/Wine Test Set.csv")
wine_train <- read.csv("/Users/cherokeecarr/Desktop/Wine Train.csv")


# Check if columns in wine_train and wine_test match, handle missing columns in wine_test
missing_columns <- setdiff(colnames(wine_train), colnames(wine_test))
wine_test[missing_columns] <- NA
colnames(wine_test) <- colnames(wine_train)

wine_test <- na.omit(wine_test)
wine_train <- na.omit(wine_train)

# Combine wine_train and wine_test into Vino
Vino <- rbind(wine_train, wine_test)

# Check the first few rows of Vino
head(Vino)

# Check for NA values
sum(is.na(Vino))
wine_test$quality <- factor(wine_test$quality)



# Remove rows with NA values (cleaning the combined dataset)
Vino_clean <- na.omit(Vino)

# Train the Naive Bayes model
nb_model <- naiveBayes(quality ~ density + pH + chlorides + fixed.acidity + volatile.acidity +
                        residual.sugar + citric.acid + sulphates + alcohol, data = Vino_clean)

# Print the Naive Bayes model summary
print(nb_model)

predictions <- predict(nb_model, newdata = wine_test)
print(predictions)



```
```{r}
# Load necessary libraries
library(ggplot2)

# List of features to plot against 'quality'
factors <- c("density", "pH", "chlorides", "fixed.acidity", 
             "volatile.acidity", "residual.sugar", "citric.acid", 
             "sulphates", "alcohol")

# Loop over the factors and create scatterplots
for (factor in factors) {
  p <- ggplot(Vino_clean, aes_string(x = factor, y = "quality")) +
    geom_point() +
    labs(title = paste("Quality vs", factor), x = factor, y = "Quality") +
    theme_minimal()
  print(p)
}

```

